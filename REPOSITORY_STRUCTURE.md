# Repository Structure and Organization Guide

## Clean Repository Structure

```
Origin-Medical/
â”‚
â”œâ”€â”€ README.md                          # Project overview and quick start
â”œâ”€â”€ PROJECT_REPORT.md                  # Comprehensive project documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ config.py                          # Central configuration file
â”‚
â”œâ”€â”€ train.sh                          # Symlink to training script
â”œâ”€â”€ evaluate.sh                       # Symlink to evaluation script
â”œâ”€â”€ inference.sh                      # Symlink to inference script
â”‚
â”œâ”€â”€ docs/                             # ğŸ“š All Documentation
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â”œâ”€â”€ APPROACH_AND_METHODOLOGY.md
â”‚   â”œâ”€â”€ PART_B_SEGMENTATION_GUIDE.md
â”‚   â””â”€â”€ UNET_EVALUATION_GUIDE.md
â”‚
â”œâ”€â”€ models/                           # ğŸ§  Model Architectures
â”‚   â”œâ”€â”€ landmark_detection/
â”‚   â”‚   â”œâ”€â”€ coordinate_regression.py  # Direct coordinate prediction
â”‚   â”‚   â”œâ”€â”€ heatmap_model.py         # Heatmap-based detection
â”‚   â”‚   â””â”€â”€ attention_pyramid.py      # Attention-based architecture
â”‚   â””â”€â”€ segmentation/
â”‚       â”œâ”€â”€ unet.py                   # U-Net for segmentation
â”‚       â””â”€â”€ deeplabv3.py              # DeepLabV3+ architecture
â”‚
â”œâ”€â”€ data/                             # ğŸ“Š Data Processing
â”‚   â”œâ”€â”€ dataset.py                    # PyTorch Dataset classes
â”‚   â”œâ”€â”€ preprocessing.py              # Image preprocessing
â”‚   â”œâ”€â”€ augmentation.py               # Data augmentation pipeline
â”‚   â””â”€â”€ augmented/                    # Augmented dataset storage
â”‚       â”œâ”€â”€ augmented_ground_truth.csv
â”‚       â”œâ”€â”€ train_split.csv
â”‚       â”œâ”€â”€ val_split.csv
â”‚       â””â”€â”€ images/
â”‚
â”œâ”€â”€ utils/                            # ğŸ› ï¸ Utility Functions
â”‚   â”œâ”€â”€ metrics.py                    # Evaluation metrics
â”‚   â”œâ”€â”€ visualization.py              # Visualization utilities
â”‚   â”œâ”€â”€ ellipse_fitting.py            # Ellipse fitting for landmarks
â”‚   â””â”€â”€ visualise_segmentation.py     # Segmentation visualization
â”‚
â”œâ”€â”€ scripts/                          # ğŸ“œ Organized Scripts
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ start_training.sh
â”‚   â”‚   â””â”€â”€ start_train.sh
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluate_coordinate.sh
â”‚   â”‚   â”œâ”€â”€ evaluate_unet.sh
â”‚   â”‚   â”œâ”€â”€ run_inference.sh
â”‚   â”‚   â”œâ”€â”€ debug_inference_pipeline.py
â”‚   â”‚   â””â”€â”€ save_predictions_to_csv.py
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ visualize_augmented_data.sh
â”‚   â”‚   â”œâ”€â”€ visualize_original_data.sh
â”‚   â”‚   â”œâ”€â”€ visualize_evaluation.py
â”‚   â”‚   â”œâ”€â”€ visualize_predictions_on_images.py
â”‚   â”‚   â”œâ”€â”€ visualize_gt_pred_overlay.py
â”‚   â”‚   â””â”€â”€ visualize_augmented_dataset.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ augment_data.sh
â”‚       â””â”€â”€ augment_dataset.py
â”‚
â”œâ”€â”€ train_landmark.py                 # Main training script (Part A)
â”œâ”€â”€ train_segmentation.py             # Segmentation training (Part B)
â”œâ”€â”€ evaluate_coordinate.py            # Coordinate model evaluation
â”œâ”€â”€ inference_landmark.py             # Inference script
â”‚
â”œâ”€â”€ checkpoints/                      # ğŸ’¾ Model Checkpoints
â”‚   â”œâ”€â”€ coordinate/
â”‚   â”‚   â”œâ”€â”€ best/
â”‚   â”‚   â”‚   â””â”€â”€ coordinate_efficientnet_b3_best_epoch=48_val_mre_overall_px=164.88.ckpt
â”‚   â”‚   â””â”€â”€ periodic/
â”‚   â””â”€â”€ heatmap/
â”‚       â”œâ”€â”€ best/
â”‚       â””â”€â”€ periodic/
â”‚
â”œâ”€â”€ results/                          # ğŸ“ˆ Outputs and Results
â”‚   â”œâ”€â”€ coordinate_evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluation_results.csv
â”‚   â”‚   â”œâ”€â”€ evaluation_summary.csv
â”‚   â”‚   â””â”€â”€ performance_dashboard.png
â”‚   â”œâ”€â”€ unnet_e48/
â”‚   â”‚   â””â”€â”€ visualise/              # GT vs Prediction visualizations
â”‚   â””â”€â”€ predictions_vs_gt.csv       # Complete predictions dataset
â”‚
â”œâ”€â”€ logs/                             # ğŸ“ Training Logs
â”‚   â”œâ”€â”€ coordinate_efficientnet_b3/
â”‚   â””â”€â”€ heatmap_resnet50/
â”‚
â”œâ”€â”€ images/                           # ğŸ–¼ï¸ Original Dataset Images
â”‚   â””â”€â”€ [ultrasound images]
â”‚
â”œâ”€â”€ originMedical/                    # ğŸ Python Virtual Environment
â”‚   â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ pyvenv.cfg
â”‚
â”œâ”€â”€ archived/                         # ğŸ“¦ Old/Temporary Files
â”‚   â””â”€â”€ temp.txt
â”‚
â”œâ”€â”€ train_split.csv                   # Training set split
â”œâ”€â”€ val_split.csv                     # Validation set split
â””â”€â”€ role_challenge_dataset_ground_truth.csv  # Original annotations
```

## File Categories

### Core Training Files
- `train_landmark.py` - Main training entry point for landmark detection
- `train_segmentation.py` - Segmentation model training
- `config.py` - Centralized configuration

### Evaluation Files
- `evaluate_coordinate.py` - Comprehensive model evaluation
- `inference_landmark.py` - Run inference on new images
- `scripts/evaluation/` - Evaluation utilities

### Data Files
- `data/dataset.py` - Dataset loaders
- `data/preprocessing.py` - Preprocessing pipeline
- `data/augmentation.py` - Augmentation strategies

### Model Definitions
- `models/landmark_detection/` - Landmark detection architectures
- `models/segmentation/` - Segmentation architectures

### Utilities
- `utils/metrics.py` - Evaluation metrics (MRE, Dice, IoU)
- `utils/visualization.py` - Plotting and visualization
- `utils/ellipse_fitting.py` - Geometric fitting utilities

### Documentation
- `README.md` - Main project README
- `PROJECT_REPORT.md` - Comprehensive project report
- `docs/` - Additional documentation

## Organization Script

Run the organization script to clean up the repository:

```bash
chmod +x organize_repo.sh
./organize_repo.sh
```

This will:
1. Create organized directory structure
2. Move files to appropriate locations
3. Create convenience symlinks
4. Archive temporary files

## Convenience Commands

After organization, use these shortcuts:

```bash
# Training
./train.sh

# Evaluation
./evaluate.sh checkpoints/coordinate/best/checkpoint.ckpt

# Inference
./inference.sh
```

## Git Ignore Recommendations

Add to `.gitignore`:

```
# Python
__pycache__/
*.py[cod]
*.so
*.egg-info/

# Virtual Environment
originMedical/
venv/
env/

# Checkpoints (too large)
checkpoints/*/periodic/
*.ckpt

# Results (generated)
results/
output/
logs/

# Temporary
temp.txt
*.tmp
.DS_Store

# Data (too large, host separately)
images/
data/augmented/images/
```

## Best Practices

### For Development:
1. Keep root directory clean
2. Use symlinks for frequently used scripts
3. Organize by function (training, evaluation, visualization)
4. Document everything in `docs/`

### For Collaboration:
1. Use version control (git)
2. Document changes in commit messages
3. Keep `requirements.txt` updated
4. Use configuration files, not hardcoded paths

### For Production:
1. Separate config for different environments
2. Use environment variables for sensitive data
3. Containerize (Docker) for deployment
4. Version your models (MLflow, DVC)

## Quick Reference

### Training a Model
```bash
python train_landmark.py --model coordinate --epochs 150 --batch_size 16
```

### Evaluating a Model
```bash
python scripts/evaluation/evaluate_coordinate.py \
    --checkpoint checkpoints/coordinate/best/checkpoint.ckpt \
    --save_visualizations
```

### Creating Visualizations
```bash
python scripts/visualization/visualize_gt_pred_overlay.py \
    --checkpoint checkpoints/coordinate/best/checkpoint.ckpt \
    --num_samples 30
```

### Analyzing Results
```bash
python scripts/evaluation/save_predictions_to_csv.py \
    --checkpoint checkpoints/coordinate/best/checkpoint.ckpt
```

## Maintenance

### Regular Tasks:
- [ ] Clean up `results/` periodically
- [ ] Archive old checkpoints
- [ ] Update documentation with changes
- [ ] Run tests before pushing code
- [ ] Review and update `requirements.txt`

### Before Sharing:
- [ ] Remove temporary files
- [ ] Check all paths are relative
- [ ] Update README with any new dependencies
- [ ] Ensure scripts are executable
- [ ] Test on fresh environment
